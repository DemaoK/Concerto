{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc78f67",
   "metadata": {},
   "source": [
    "# Getting Started with Milky Way-est Analysis\n",
    "\n",
    "*last updated -- july 2024*\n",
    "\n",
    "*written by -- deveshi buch*\n",
    "\n",
    "*paper authors -- deveshi buch, ethan o. nadler, risa h. wechsler, yao-yuan mao*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962f853",
   "metadata": {},
   "source": [
    "Welcome, and thanks for your interest in using the Milky Way-est suite of dark matter halos! As a reminder, this suite contains Milky Way-like host halos selected to have an LMC-like subhalo at present and a GSE-like subhalo that merged with it billions of years ago.\n",
    "\n",
    "For more information on Milky Way-est, please see the paper, Buch et al. 2024. If you have questions about this notebook, you can contact the authors.\n",
    "\n",
    "This notebook will show you how to load information from the halo catalogs (i.e., the `hlists` and `trees` files).\n",
    "\n",
    "And a note before getting started: `mwest` will be often used to refer to the suite!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affad464",
   "metadata": {},
   "source": [
    "## Step 0: Data access\n",
    "\n",
    "This notebook assumes that you have access to the Milky Way-est dataset.\n",
    "\n",
    "**Important:** The data that this notebook uses are the halo catalog files (called `hlists` and `trees`) directly rather than the `.tar` files that are `symlib` compatible.\n",
    "\n",
    "Once you gain access to these files, you will want to download them to your preferred location. You can use whichever method of downloading you prefer, and you can consult the Data Access Instructions page for download methods (these are written for `symlib` compatible files but you can mirror the `rclone` approach for Milky Way-est)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dba326",
   "metadata": {},
   "source": [
    "## Step 1: Setting up\n",
    "\n",
    "Firstly, if you're reading this, you likely have access to Python and Jupyter. The next step is making sure you have [Yao-Yuan Mao's `helpers`](https://bitbucket.org/yymao/helpers/src/master/), which we use to load the halo catalogs.\n",
    "\n",
    "Uncomment the line below to install it, or paste it (without the `!` at the beginning) into your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://bitbucket.org/yymao/helpers/get/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc207203",
   "metadata": {},
   "source": [
    "It would also be a good idea to have packages like NumPy, SciPy, Pandas, Seaborn, Matplotlib, etc. loaded on your machine too, if you don't already! In particular, we use [NumPy](https://numpy.org/install/), [Pandas](https://pandas.pydata.org/docs/getting_started/install.html), and [Matplotlib](https://matplotlib.org/stable/install/index.html) in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02eeb6",
   "metadata": {},
   "source": [
    "Below, we import a few scripts with some useful functions from `helpers`, define the Hubble parameter `h`, and set up our file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.SimulationAnalysis import SimulationAnalysis, readHlist, a2z\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "h = 0.7  # note: don't use the variable name h anywhere else!\n",
    "\n",
    "#######\n",
    "# TODO: update this path to point the location that you have the data downloaded to\n",
    "######\n",
    "\n",
    "MAIN_ZOOMIN_DIR = '/path/to/where/your/directory/of/mwest/halos/is/downloaded'  # TODO: fill this in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ed1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get halo information\n",
    "halo_info_df = pd.read_csv('mwest_halo_info.csv')\n",
    "\n",
    "halo_names = list(halo_info_df['halo_name'])\n",
    "host_analysis_timestep_str_arr = list(halo_info_df['snapshot_to_analyze'])  # z \\approx 0 as described in Buch et al 2024 \n",
    "host_tree_root_id_arr = list(halo_info_df['host_tree_root_id'])  # treerootid of GSE\n",
    "lmc_tree_root_id_arr = list(halo_info_df['lmc_tree_root_id'])  # treerootid of LMC\n",
    "gse_disrupt_id_arr = list(halo_info_df['gse_disrupt_id'])  # disruption timestep of GSE\n",
    "\n",
    "mwest_host_treerootids = {key : val for (key,val) in zip(halo_names, host_tree_root_id_arr)}\n",
    "mwest_lmc_treerootids = {key : val for (key,val) in zip(halo_names, lmc_tree_root_id_arr)}\n",
    "mwest_gse_disrupt_ids = {key : val for (key,val) in zip(halo_names, gse_disrupt_id_arr)}\n",
    "mwest_analysis_timesteps_str = {key : val for (key,val) in zip(halo_names, host_analysis_timestep_str_arr)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb7bc9",
   "metadata": {},
   "source": [
    "## Step 2: Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3b2f5",
   "metadata": {},
   "source": [
    "### Main branches\n",
    "\n",
    "Let's go ahead and load the main branches of our halos (and their LMCs). The main branches trace each of the Milky Way-est halos back in time, for each timestep the halo exists, and the halo catalogs contain information about each halo at each of these timesteps. You can use this information in any analysis you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb73c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this cell will take a few minutes to run!\n",
    "\n",
    "mwest_host_mbs = {}  # dictionary where the key will be halo name and the value will be its main branch\n",
    "mwest_lmc_mbs = {}\n",
    "\n",
    "for this_zoomin in halo_names:\n",
    "    \n",
    "    zoomin_dir = os.path.join(MAIN_ZOOMIN_DIR, this_zoomin)\n",
    "\n",
    "    hlist_dir = os.path.join(zoomin_dir, 'hlists/')\n",
    "    trees_dir = os.path.join(zoomin_dir, 'trees/')\n",
    "    \n",
    "    resim = SimulationAnalysis(hlist_dir, trees_dir)\n",
    "    mwest_host_mbs[this_zoomin] = resim.load_main_branch(mwest_host_treerootids[this_zoomin])\n",
    "    mwest_lmc_mbs[this_zoomin] = resim.load_main_branch(mwest_lmc_treerootids[this_zoomin])\n",
    "\n",
    "print(mwest_host_mbs['Halo004'].dtype.names)\n",
    "print(mwest_host_mbs['Halo004'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e0434",
   "metadata": {},
   "source": [
    "The main branch of Halo004, as can be seen above, has many fields, and we can directly use them for analysis. Note that the `id` of the halo changes at each timestep. Here's the plot of the Halo004 host mass accretion history (MAH) for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd05555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mwest_host_mbs['Halo004']['scale'], mwest_host_mbs['Halo004']['mvir'] / h)\n",
    "plt.xlabel('scale factor a')\n",
    "plt.ylabel('virial mass (Msun)')\n",
    "plt.title('mass accretion history')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb8fc6",
   "metadata": {},
   "source": [
    "**Important notes on units:**\n",
    "\n",
    "- Check out the ROCKSTAR documentation for detailed information: https://bitbucket.org/gfcstanford/rockstar/src/main/README.pdf\n",
    "- Distances are comoving (not physical), so if you want to use the virial radius Rvir, for example, you'll want to divide it by the scale factor at which you're analyzing that Rvir.\n",
    "- Many parameters have a factor of h (the Hubble parameter, which we assume to be 0.7) that needs to be removed in order to get the raw units of the parameter. For example, Mvir is the virial mass in units of Msun, but the 'mvir' field is in units of Msun/h. So, don't forget to divide by h to remove this factor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94bd3c",
   "metadata": {},
   "source": [
    "### Halos and subhalos\n",
    "\n",
    "Suppose we want to load all the halos at a given snapshot, or the subhalos of one of our Milky Way-est hosts. Here's how we'd do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb24eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example using Halo004, at its analysis timestep (which happens to be right at z=0 or a=1)\n",
    "\n",
    "zoomin_of_interest = 'Halo004'\n",
    "timestep_of_interest = float(mwest_analysis_timesteps[zoomin_of_interest].strip('\"'))\n",
    "\n",
    "print('zoomin of interest: ' + str(zoomin_of_interest))\n",
    "print('timestep of interest: ' + str(timestep_of_interest))\n",
    "\n",
    "zoomin_dir = os.path.join(MAIN_ZOOMIN_DIR, zoomin_of_interest)\n",
    "\n",
    "hlist_dir = os.path.join(zoomin_dir, 'hlists/')\n",
    "trees_dir = os.path.join(zoomin_dir, 'trees/')\n",
    "\n",
    "# get all halos\n",
    "resim = SimulationAnalysis(hlist_dir, trees_dir)\n",
    "all_halos_at_this_snapshot = resim.load_halos(z=a2z(timestep_of_interest), additional_fields=['vacc', 'mpeak', 'macc'])\n",
    "print(\"number of halos found: \" + str(len(all_halos_at_this_snapshot)))\n",
    "\n",
    "# get only halos that are large enough given the resolution limit\n",
    "halos_above_resolution_limit = all_halos_at_this_snapshot[np.where(all_halos_at_this_snapshot['mvir']/h > 1.2e8)[0]]\n",
    "print(\"number of halos above the resolution limit: \" + str(len(halos_above_resolution_limit)))\n",
    "\n",
    "# get subhalos of a given halo\n",
    "timestep_of_interest_index = np.argmin(np.abs(mwest_host_mbs[zoomin_of_interest]['scale'] - timestep_of_interest))\n",
    "zoomin_of_interest_id = mwest_host_mbs[zoomin_of_interest][timestep_of_interest_index]['id']\n",
    "subhalos_of_zoomin_at_this_timestep = halos_above_resolution_limit[np.where(halos_above_resolution_limit['upid'] == zoomin_of_interest_id)[0]]\n",
    "print(\"number of subhalos of zoomin of interest: \" + str(len(subhalos_of_zoomin_at_this_timestep)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fd1a8",
   "metadata": {},
   "source": [
    "**Note on resolution:** The mass of the highest-resolution particles is 4.0 x 10^5 Msun. This means that, with a 300-particle resolution limit, our subhalo analysis should only include subhalos with masses above 4.0 x 10^5 Msun x 300 = 1.2 x 10^8 Msun. If this resolution cut is applicable to you, we suggest you incorporate it into your pipeline! (Otherwise, the halos pulled directly from the halo catalogs without this additional cut will include halos with masses below this threshold.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfcc6b",
   "metadata": {},
   "source": [
    "### Other tips\n",
    "\n",
    "In addition to the very important notes above (which you should go back and read if you skipped over them), some things to keep in mind:\n",
    "\n",
    "- Occasionally different ways of loading in halos will result in slightly different namings of the parameters (e.g., 'Mvir' instead of 'mvir'), so if you're ever confused about why a field seems to be missing when it should be included in your main branch, just check what the labels are!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad550e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
